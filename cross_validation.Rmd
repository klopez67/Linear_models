---
title: "Cross Validation"
author: "Kimberly Lopez"
date: "2024-11-14"
output: github_document
---

```{r}
library(tidyverse)
library(modelr)
library(mgcv)

library(SemiPar)

set.seed(1)
```

working with the Lidar dataset 

```{r}
data("lidar")

lidar_df = 
  lidar|>
  as_tibble()|>
  mutate( id= row_number())

```

Graphing the Lidar data: 
```{r}
lidar_df|>
  ggplot( aes(x= range, y = logratio))+ 
  geom_point()
```

**Is this model too complex to fit a linear relationship?** 

# Cross Validation 

We will compare 3 models, linear, smooth, wiggly 

Frist, contruct training and testing data 
```{r}

train_df = sample_frac(lidar_df, size = .8)

test_df = anti_join(lidar_df, train_df, by = "id")

```

Visualizing Training data: 
```{r}
ggplot(train_df, aes(x= range, y = logratio))+ 
  geom_point()+ 
  geom_point(data = test_df, color = "red")
```

Now we can start fitting one model: 
--> one model will be too simple or too complex and make incorrect predictions. 

Three models: 
```{r}
linear_mod= lm(logratio ~ range, data = train_df)
smooth_mod= gam(logratio ~ s(range), data = train_df)
wiggly_mod = gam(logratio ~ s(range, k = 30, sp= 10e-6), data = train_df)
```

Visualize the model fits: 

**Linear fit** 
```{r}
train_df|>
  add_predictions(linear_mod)|> 
  ggplot(aes(x=range, y = logratio))+ 
  geom_point() + 
  geom_line(aes(y = pred), color = "red")
```

- This is not complex enough since some of the datapoints are not linear 

**Wiggly model fit** 
```{r}
train_df|>
  add_predictions(wiggly_mod)|> 
  ggplot(aes(x=range, y = logratio))+ 
  geom_point() + 
  geom_line(aes(y = pred), color = "red")
```

- this over fits the training data on the model, so we cannot generalize this for a different dataset 

**Smooth model fit** 
```{r}
train_df|>
  add_predictions(smooth_mod)|> 
  ggplot(aes(x=range, y = logratio))+ 
  geom_point() + 
  geom_line(aes(y = pred), color = "red")
```

- this is better than linear since it captures the trends in the data, but not too flexible that will cause overfitting 

**Compare these numerically using RMSE** 

```{r}

rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```

Smooth_mod has lower RMSE 

We want to see if this differece is consistent for random data testing 

# Repeat the Train/test split 
